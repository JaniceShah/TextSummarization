# -*- coding: utf-8 -*-
"""Similarity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TOr_mC4DopXMbsBvPhDGbIjBQxrC-7OK
"""

from absl import logging

import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import re
import seaborn as sns
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

module_url = "https://tfhub.dev/google/universal-sentence-encoder-large/5" #@param ["https://tfhub.dev/google/universal-sentence-encoder/2", "https://tfhub.dev/google/universal-sentence-encoder-large/3","https://tfhub.dev/google/universal-sentence-encoder-large/5"]
embed = hub.load(module_url)

def plot_similarity(labels,su, features,sp, rotation,i):
    corr = np.inner(features, features)
    z=0;
    for j in range(0,i):
        if(corr[j][i]>0.8 and sp[j]==sp[i]):
            z=1

    if(z==0):
        final.append(labels[i])
        summ.append(su[i])
    sns.set(font_scale=1.2)
    g = sns.heatmap(
        corr,
        xticklabels=labels,
        yticklabels=labels,
        vmin=0,
        vmax=1,
        cmap="YlOrRd")
    g.set_xticklabels(labels, rotation=rotation)
    g.set_title("Semantic Textual Similarity")


def run_and_plot(session_, input_tensor_, messages_,su, sp_, encoding_tensor,i):
    message_embeddings_ = session_.run(
        encoding_tensor, feed_dict={input_tensor_: messages_})
    plot_similarity(messages_,su, message_embeddings_,sp_,90,i)

"""messages = [
    # Smartphones
    "I like my phone",
    "My phone is not good.",
    "Your cellphone looks great.",

    # Weather
    "Will it snow tomorrow?",
    "Recently a lot of hurricanes have hit the US",
    "Global warming is real",

    # Food and health
    "An apple a day, keeps the doctors away",
    "Eating strawberries is healthy",
    "Is paleo better than keto?",

    # Asking about age
    "How old are you?",
    "what is your age?",
]

with tf.Session() as session:
    session.run(tf.global_variables_initializer())
    session.run(tf.tables_initializer())
    x= len(messages)
    final.append(messages[0])
    for i in range(1,x):
        print(messages[:i+1])
        run_and_plot(session, similarity_input_placeholder, messages[:i+1],similarity_message_encodings,i)

#with tf.Session() as session:
#     session.run([tf.global_variables_initializer(), tf.tables_initializer()])
#     message_embeddings = session.run(embed(messages))

#     for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):
#         print("Message: {}".format(messages[i]))
#         print("Embedding size: {}".format(len(message_embedding)))
#         message_embedding_snippet = ", ".join(
#             (str(x) for x in message_embedding[:3]))
#         print("Embedding: [{}, ...]\n".format(message_embedding_snippet))

print(final)
"""

from google.colab import drive
drive.mount('/content/drive')

rev=pd.read_csv(r"/content/drive/My Drive/MP2/T3.csv",encoding='latin1')

X=rev[['Statement','Analysis']]
Y=rev[['Summary']]

X = X.dropna(how='any',axis=0)

Y.head()

import nltk
nltk.download('stopwords')

import spacy
import nltk
from nltk.tokenize.toktok import ToktokTokenizer
import re
from bs4 import BeautifulSoup
import unicodedata
from nltk.corpus import stopwords

nlp = spacy.load('en_core_web_sm', parse = False, tag=False, entity=False)
tokenizer = ToktokTokenizer()
stopword_list = nltk.corpus.stopwords.words('english')
stopword_list.remove('no')
stopword_list.remove('not')

def strip_html_tags(text):
    soup = BeautifulSoup(text, "html.parser")
    stripped_text = soup.get_text()
    return stripped_text

def remove_accented_chars(text):
    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
    return text

def remove_special_characters(text):
    text = re.sub(r'https?:\/\/.\*[\r\n]*', '', text, flags=re.MULTILINE)
    text = re.sub(r'\<a href', ' ', text)
    text = re.sub(r'&amp;', '', text)
    text = re.sub(r'[_"\-;%()|+&=*%,!?:#$@\[\]/]', ' ', text)
    text = re.sub(r'<br />', ' ', text)
    text = re.sub(r'<br >', ' ', text)
    text = re.sub(r'<br  >', ' ', text)
    text = re.sub(r'\'', ' ', text)
    return text

"""def lemmatize_text(text):
    text = nlp(text)
    print(text)
    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])
    return text
"""

def remove_stopwords(text, is_lower_case=False):
    tokens = tokenizer.tokenize(text)
    tokens = [token.strip() for token in tokens]
    filtered_tokens = [token for token in tokens if token not in stopword_list]
    filtered_text = ' '.join(filtered_tokens)
    return filtered_text

def clean_text(text, remove):
    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''

    # Convert words to lower case
    text = text.lower()
    # Format words and remove unwanted characters
    text=remove_special_characters(text)


    return text

clean_texts = []
clean_sum=[]
for text,summ in zip(X['Statement'],Y['Summary']):
    clean_texts.append(str(text))
    clean_sum.append(str(summ))
print("Texts are complete.")

print(len(X),len(Y))

for i in range(0,5):
    print("Text: ", clean_texts[i])
    print()

X['Statement'].head()

final=[]
summ=[]



logging.set_verbosity(logging.ERROR)
similarity_input_placeholder = tf.placeholder(tf.string, shape=(None))
similarity_message_encodings = embed(similarity_input_placeholder)
with tf.Session() as session:
    session.run(tf.global_variables_initializer())
    session.run(tf.tables_initializer())
    p= len(clean_texts)
    final.append(clean_texts[0])
    summ.append(clean_sum[0])
    for i in range(1,p):
        run_and_plot(session, similarity_input_placeholder, clean_texts[:i+1],clean_sum[:i+1],X['Analysis'], similarity_message_encodings,i)

print(len(final),len(summ))

fin = pd.DataFrame(list(zip(final, summ)),
               columns =['Review', 'Summary'])

